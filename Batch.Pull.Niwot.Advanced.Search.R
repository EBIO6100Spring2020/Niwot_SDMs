

#Here we read in the package Ids file that the python scraper generated. So you'll need to run python
#script from the same directory that this file is in, or else this won't work.

pids <- read.table("packageids.txt")

#Same for this file, which should also be generated by the python scraper. These study names will be used
#to give informative names to the csv files we generate further in. BIG WARNING HERE. If the projects you scrape
#have names that contain commas somewhere other than between the study name and the year (or they dont have
#a comma between name and year) then this will give you some weird results for file names. It doesn't break the
#script or anything, but it may defeat the purpose of renaming with an informative title. So keep an eye on that.

study_names <- read.table("study_names.txt", sep=",")

#here we replace all the spaces with underscores because I'm not a monster.
study_names$V1 <- gsub(" ","_",study_names$V1)


#The big issue with pulling LTER Net data is that you don't use the package ID (like knb-lter-nwt.120.1 for
#a niwot study with ID 120 version 1). Instead it uses a much longer alpha numeric ID called a PASTA ID.
#Getting that is logistically miserable from scraping, but as it turns out if you query the site to download
#using just the package ID, it very conveniently returns the PASTA ID for all associated data files. So in 
#the upcoming loop we download PASTA IDs and paste them into a full URL as well as generating the file
#name we will eventually use for each data file we want.

#create a file name to keep our pasta ID in and initialize some vectors or whatever
infile1 <- "Pasta.Ids.txt"
full_url <- c()
next_file <- c()
final_file <- c()
#we also need an external iterator just in case we have any package IDs with multiple data files attached. Long
#story there, but this works, probably.
iter <- 1
for(i in 1:length(pids$V1)){
#minor issue here is that our package IDs use like knb-lter-nwt.120.1 but the directory structure on the server 
  #obviously uses nwt/120/1 so we just replace the . with a /
  temp_name <- gsub("\\.","/",pids$V1[i])
  
  #paste the id onto the url, still now lacking our pasta id
  u_r <- paste("https://pasta.lternet.edu/package/data/eml/",temp_name,sep="")
  
  #download the info at that url, which (hopefully) is our pasta ID. This saves it to a file. I have yet to
  #figure out how to download it into R's working memory. If you figure out how to do that, lemme know.
  download.file(u_r,infile1,method="curl")
  #read that pasta ID in
  dumb_id <- read.table(infile1)
  
  #some data packages have multiple data files associated with a given ID. As a result, when you get the PASTA ID,
  #it will return more than one. Here we check to make sure we don't have more than one. If we do, we loop through
  #them, creating a URL for each, naming a file, and continuing to update the iterator we use to index each
  #object.
  if(length(dumb_id$V1)>1){
    for(j in 1:length(dumb_id$V1)){
      full_url[iter] <- paste(u_r,dumb_id$V1[j],sep="/")
      #set up the eventual file we want, named after the package ID
      next_file[iter] <- paste(getwd(),study_names$V1[i],sep="/")
      #add CSV to that final file name
      final_file[iter] <- paste(next_file[iter],".",j,".csv",sep="")
      iter <- iter+1
    }

   
  }else{ #if we don't have more than one PASTA for a given package, just create the URL and file name
    #create the full URL with the pasta ID separated by a slash
    full_url[iter] <- paste(u_r,dumb_id$V1,sep="/")
    #set up the eventual file we want, named after the package ID
    next_file[iter] <- paste(getwd(),study_names$V1[i],sep="/")
    #add CSV to that final file name
    #we need two calls here because creating the directory path uses /, whereas adding the .csv won't use any
    #separator.
    final_file[iter] <- paste(next_file[i],".csv",sep="")

    #update that iterator pls
    iter <- iter+1
  }
}


# multi_url <- "https://pasta.lternet.edu/package/data/eml/knb-lter-nwt/210/1"
# download.file(multi_url, "Another.test.txt",method="curl")

#now we download the actual data. I separated this into its own loop but you might not have to? Who knows.
#things got messy while testing this but you might be able to just throw this back into the original loop.
#helps to have two to troubleshoot though, as running this takes a lot longer than just pulling PASTA Ids.
for(i in 1:length(full_url)){
  download.file(full_url[i],final_file[i],method="curl")
}

length(full_url)
final_file
